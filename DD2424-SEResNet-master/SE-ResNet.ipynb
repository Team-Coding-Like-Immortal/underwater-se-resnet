{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kill' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "#%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "!kill $(ps aux | grep './ngrok' | awk '{print $2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-08dfcfb8f705>\", line 3, in <module>\n",
      "    import tensorflow_addons as tfa\n",
      "ModuleNotFoundError: No module named 'tensorflow_addons'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ModuleNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"c:\\program files\\python37\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"c:\\program files\\python37\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"c:\\program files\\python37\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"c:\\program files\\python37\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\43459\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\43459\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"c:\\program files\\python37\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_addons'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from ResNet50 import *\n",
    "from LoadDataset import *\n",
    "from tensorboard import notebook\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "#config = tf.compat.v1.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(2)\n",
    "tf.print(a)\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Reshape\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow.keras.backend as K\n",
    "from Squeeze_and_Excite import Squeeze_and_Excite \n",
    "from ResNet50 import *\n",
    "from LoadDataset import *\n",
    "\n",
    "#%load_ext tensorboard\n",
    "#!rm -rf ./logs/\n",
    "\n",
    "epoch = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "dataset = 10\n",
    "momentum = 0.9\n",
    "wd = 1e-2\n",
    "\n",
    "#lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.1, decay_steps=100000, decay_rate=0.9)\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=momentum)\n",
    "#optimizer = tfa.optimizers.SGDW(learning_rate=lr_schedule, weight_decay=wd, momentum=momentum)\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, decay=decay_rate, momentum=momentum)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer_adam = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "optimizer_sgd1 = tf.keras.optimizers.SGD(learning_rate=learning_rate/10)\n",
    "optimizer_sgd2 = tf.keras.optimizers.SGD(learning_rate=learning_rate/100)\n",
    "optimizer_sgd3 = tf.keras.optimizers.SGD(learning_rate=learning_rate/1000)\n",
    "optimizer = optimizer_adam\n",
    "#optimizer = tfa.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=wd)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "test_top1 = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1, name='test_1_accuracy') \n",
    "test_top5 = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='test_5_accuracy') \n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "        #K.set_learning_phase(1)\n",
    "        predictions = model(images, training = True)\n",
    "        loss =loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "        \n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    #K.set_learning_phase(0)\n",
    "    predictions = model(images, training = False)\n",
    "    t_loss =loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "    test_top1(labels, predictions)\n",
    "    test_top5(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "Loading ./Datasets/cifar-10-batches-py/data_batch_1 : 10000.\n",
      "Loading ./Datasets/cifar-10-batches-py/data_batch_2 : 10000.\n",
      "Loading ./Datasets/cifar-10-batches-py/data_batch_3 : 10000.\n",
      "Loading ./Datasets/cifar-10-batches-py/data_batch_4 : 10000.\n",
      "Loading ./Datasets/cifar-10-batches-py/data_batch_5 : 10000.\n",
      "Loading ./Datasets/cifar-10-batches-py/test_batch : 10000.\n",
      "Train data: (50000, 32, 32, 3) (50000,)\n",
      "Test data : (10000, 32, 32, 3) (10000,)\n",
      "normalizing data...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    save_model_dir = \"./checkpoints_norm\"\n",
    "\n",
    "    print(\"loading data ...\")\n",
    "    train_X, train_lab, test_X, test_lab = get_data(dataset)\n",
    "    print(\"normalizing data...\")\n",
    "    train_X, test_X = normalize(train_X, test_X)\n",
    "    #print(\"augmenting data...\")\n",
    "    #train_X = data_augmentation(train_X)\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((train_X, train_lab)).batch(batch_size)\n",
    "    test_data = tf.data.Dataset.from_tensor_slices((test_X, test_lab)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #model = ResNet50(include_top=True, weights=None, squeeze=False, squeeze_type='Normal')#, input_tensor = tf.data.Dataset.from_tensor_slices((train_X)))\n",
    "    #input_tensor = tf.placeholder(tf.float32, shape = [None, train_X.shape[0], train_X.shape[1], train_X.shape[2]])\n",
    "    model = ResNet50(include_top = True, squeeze = True, squeeze_type = 'pre', classes = dataset) #pre, identity, normal\n",
    "\n",
    "    \"\"\"\n",
    "    features, label = iter(train_dataset).next()\n",
    "    print(\"example features:\", features[0])\n",
    "    print(\"example label:\", label[0])\n",
    "    \"\"\"\n",
    "    \n",
    "    checkpoint_dir = os.path.join(save_model_dir, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)\n",
    "    #checkpoint.restore(manager.latest_checkpoint)\n",
    "    #if manager.latest_checkpoint:\n",
    "    #    print(\"Restaurado de {}\".format(manager.latest_checkpoint))\n",
    "    #else:\n",
    "    #    print(\"Inicializando desde cero\")\n",
    "\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "    test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-13 10:19:29--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
      "Resolving bin.equinox.io (bin.equinox.io)... 54.210.240.130, 34.194.84.166, 34.205.198.58, ...\n",
      "Connecting to bin.equinox.io (bin.equinox.io)|54.210.240.130|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13773305 (13M) [application/octet-stream]\n",
      "Saving to: ‘ngrok-stable-linux-amd64.zip.7’\n",
      "\n",
      "ngrok-stable-linux- 100%[===================>]  13.13M  15.2MB/s    in 0.9s    \n",
      "\n",
      "2020-05-13 10:19:30 (15.2 MB/s) - ‘ngrok-stable-linux-amd64.zip.7’ saved [13773305/13773305]\n",
      "\n",
      "Archive:  ngrok-stable-linux-amd64.zip\n",
      "  inflating: ngrok                   \n"
     ]
    }
   ],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip -o ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://7fb33ce5.ngrok.io\n"
     ]
    }
   ],
   "source": [
    "# Run 2 times\n",
    "logdir = \"logs\"\n",
    "LOG_DIR = logdir\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")\n",
    "get_ipython().system_raw('./ngrok http 6006 &')\n",
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 1, Train Loss: 2.514732837677002, Train Accuracy: 24.336000442504883, Test Loss: 1.790527105331421, Test Accuracy: 32.21000289916992\n",
      "Top1 Error: 67.78999328613281, Top5 Error: 14.139997482299805\n",
      "1\n",
      "Epoch 2, Train Loss: 1.592466950416565, Train Accuracy: 41.198001861572266, Test Loss: 1.6893061399459839, Test Accuracy: 42.53999710083008\n",
      "Top1 Error: 62.625003814697266, Top5 Error: 11.584997177124023\n",
      "2\n",
      "Epoch 3, Train Loss: 1.3145021200180054, Train Accuracy: 52.704002380371094, Test Loss: 1.5305794477462769, Test Accuracy: 52.45000076293945\n",
      "Top1 Error: 57.599998474121094, Top5 Error: 10.250001907348633\n",
      "3\n",
      "Epoch 4, Train Loss: 1.143921136856079, Train Accuracy: 59.59600067138672, Test Loss: 1.562525749206543, Test Accuracy: 51.19000244140625\n",
      "Top1 Error: 55.40250015258789, Top5 Error: 9.882497787475586\n",
      "4\n",
      "Epoch 5, Train Loss: 1.0316218137741089, Train Accuracy: 64.49600219726562, Test Loss: 1.4670301675796509, Test Accuracy: 54.14999771118164\n",
      "Top1 Error: 53.49199676513672, Top5 Error: 9.539997100830078\n",
      "5\n",
      "Epoch 6, Train Loss: 0.9234634041786194, Train Accuracy: 68.4520034790039, Test Loss: 1.0498498678207397, Test Accuracy: 64.11000061035156\n",
      "Top1 Error: 50.55833435058594, Top5 Error: 8.684998512268066\n",
      "6\n",
      "Epoch 7, Train Loss: 0.8820085525512695, Train Accuracy: 69.80599975585938, Test Loss: 0.9963199496269226, Test Accuracy: 66.50999450683594\n",
      "Top1 Error: 48.119998931884766, Top5 Error: 8.04142951965332\n",
      "7\n",
      "Epoch 8, Train Loss: 0.7750112414360046, Train Accuracy: 73.6760025024414, Test Loss: 0.9954511523246765, Test Accuracy: 67.3499984741211\n",
      "Top1 Error: 46.186248779296875, Top5 Error: 7.537502288818359\n",
      "8\n",
      "Epoch 9, Train Loss: 0.6917347311973572, Train Accuracy: 76.62000274658203, Test Loss: 1.0110985040664673, Test Accuracy: 67.58000183105469\n",
      "Top1 Error: 44.65666580200195, Top5 Error: 7.125556468963623\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print(i)\n",
    "        if i == 20:\n",
    "            optimizer = optimizer_sgd1\n",
    "        if i == 50:\n",
    "            optimizer = optimizer_sgd2\n",
    "        if i == 80:\n",
    "            optimizer = optimizer_sgd3\n",
    "            \n",
    "        for images, labels in train_data:\n",
    "            train_step(images, labels)\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=i+1)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=i+1)\n",
    "\n",
    "            \n",
    "        for images, labels in test_data:\n",
    "            test_step(images, labels)\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=i+1)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=i+1)\n",
    "            tf.summary.scalar('top1error', (1 - test_top1.result())*100, step=i+1)\n",
    "            tf.summary.scalar('top5error', (1 - test_top5.result())*100, step=i+1)\n",
    "            \n",
    "\n",
    "        template = 'Epoch {}, Train Loss: {}, Train Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "        print(template.format(i+1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result()*100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()*100))\n",
    "\n",
    "        template = 'Top1 Error: {}, Top5 Error: {}'\n",
    "        print(template.format((1 - test_top1.result())*100,\\\n",
    "                            (1 - test_top5.result())*100))\n",
    "\n",
    "        save_path = manager.save()\n",
    "\n",
    "        train_losses.append(train_loss.result())\n",
    "        train_accs.append(train_accuracy.result())\n",
    "        test_losses.append(test_loss.result())\n",
    "        test_accs.append(test_accuracy.result())\n",
    "\n",
    "        # Reinicia las metricas para el siguiente epoch.\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "\n",
    "\n",
    "    t = np.linspace(1, epoch, num=epoch)\n",
    "    plot1 = plt.figure(1)\n",
    "    train_l = plt.plot(t, train_losses, 'b', label='training')\n",
    "    test_l = plt.plot(t, test_losses, 'r', label='validation')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(handles=[train_l, test_l])\n",
    "    plt.title('Loss evolution')\n",
    "    plt.savefig('./Result_Pics/loss_train')\n",
    "    plot2 = plt.figure(2)\n",
    "    train_a = plt.plot(t, train_accs, 'b', label='training')\n",
    "    test_a = plt.plot(t, test_accs, 'r', label='validation')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(handles=[train_l, test_l])\n",
    "    plt.title('Accuracy evolution')\n",
    "    plt.savefig('.//Result_Pics//acc_train')\n",
    "    plt.show()\n",
    "    \n",
    "    a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
